### 操作数据

------

[TOC]

##### 00：概述

- 以传统的行列的方式存储数据，这种固定的存储格式导致对象的灵活性不复存在，但是如果能以对象的格式存储数据，使用方就可以专注的使用数据，把对象本来的灵活性找回来；

##### 01：文档

- 它特指**最顶层结构**或者**根对象(root object)**序列化成的JSON数据（以唯一ID标识并存储于Elasticsearch中）
- 对象和文档还是有稍微区别的；

##### 02：文档的元数据(metadata)

| 节点     | 说明                                                         |
| -------- | ------------------------------------------------------------ |
| `_index` | 文档存储的地方【索引】，全部小写                             |
| `_type`  | **文档代表的对象的类**，相同类型的type的文档表示相同的"事物"，他们**数据结构相同** |
| `_id`    | 文档的唯一标识，**当与_index、_type 和 _id 联合使用时，就可以定位ES中唯一的文档** |

![DSL-SQL](/Users/likang/Code/Git/Middleware/Elasticsearch/photos/DSL-SQL.png)

##### 03：索引一个文档

1. 使用自定义的 ID 索引文档；

   - 在 ES 中每个文档都有一个版本号。当每次对文档进行修改时（包括删除）， `_version` 的值会递增，确保你的应用程序中的一部分修改不会覆盖另一部分所做的修改。

   ```sh
   PUT /{index}/{type}/{id}
   {
     "field": "value",
     ...
   }
   
   {
      "_index":    "website",
      "_type":     "blog",
      "_id":       "123",
      "_version":  1,
      "created":   true
   }
   ```

2. Autogenerating IDs

   - 自动生成的 ID 是 URL-safe、 基于 Base64 编码且长度为20个字符的 GUID 字符串；

   ```sh
   POST /website/blog/
   {
     "title": "My second blog entry",
     "text":  "Still trying this out...",
     "date":  "2014/01/01"
   }
   # 除了 _id 是 Elasticsearch 自动生成的，响应的其他部分和前面的类似
   {
      "_index":    "website",
      "_type":     "blog",
      "_id":       "AVFgSgVHUP18jI2wRx0w",
      "_version":  1,
      "created":   true
   }
   ```

##### 04：检索文档

- ```sh
  GET /website/blog/123?pretty
  ```

- **pretty**：返回美化的的_source值，便于阅读；

- **{"found": true}** ：用于判断有没有找到返回的值；

  ```sh
  {
    "_index" :   "website",
    "_type" :    "blog",
    "_id" :      "123",
    "_version" : 1,
    "found" :    true,
    "_source" :  {
        "title": "My first blog entry",
        "text":  "Just trying this out...",
        "date":  "2014/01/01"
    }
  }
  ```

- **-i**：curl 加 -i 参数会返回响应头，用于检查文档是否存在；

  ```sh
  curl -i -X GET http://localhost:9200/website/blog/124?pretty
  
  HTTP/1.1 404 Not Found
  Content-Type: application/json; charset=UTF-8
  Content-Length: 83
  
  {
    "_index" : "website",
    "_type" :  "blog",
    "_id" :    "124",
    "found" :  false
  }
  ```

- **_source**：当我们只需要返回指定字段时，使用此参数，多个字段用逗号分隔；

  ```sql
  GET /website/blog/123?_source=title,text
  # 只返回 _source 的值，不需要其他元数据
  ```
  
  - 如果你只想得到 `_source` 字段，不需要任何元数据，你能使用 `_source` 端点
  
  - ```
    GET /website/blog/123/_source
    ```

- 结果解析
  - _shards：头提供有关索引操作的复制过程的信息。
    1. total ： 指示索引操作应在多少个分片（ 主primary 和replica）上执行。
    2. successful：指示索引操作成功执行的分片数。
    3. failed：包含错误信息。 索引操作成功执行时，successful至少是1。

##### 05：更新整个文档

- 在 ES 中文档是不可改变的，不能修改它们。 相反，如果想要更新现有的文档，需要重建索引或者进行替换， 我们可以使用相同的index API 进行实现；
- 在内部，Elasticsearch 已将旧文档标记为已删除，并**增加一个全新的文档**。 尽管你不能再对旧版本的文档进行访问，但它并不会立即消失。当继续索引更多的数据，Elasticsearch 会在后台清理这些已删除文档。

```sql
PUT /website/blog/123
{
  "title": "My first blog entry",
  "text":  "I am starting to get the hang of this...",
  "date":  "2014/01/02"
}

{
  "_index" :   "website",
  "_type" :    "blog",
  "_id" :      "123",
  "_version" : 2,
  "created":   false
}
```

##### 06：文档的部分更新 [-update]

- `update` API 遵循同样的规则， 从外部来看，我们在一个文档的某个位置进行部分更新。然而在内部， `update`与之前描述相**同的检索-修改-重建索引的处理过程**；

- `update` 请求最简单的一种形式是**接收文档的一部分作为 `doc` 的参数， 它只是与现有的文档进行合并**。对象被合并到一起，**覆盖现有的字段，增加新的字段**；

- **路径上需要添加_update 字段，否则会更新整个文档**；

  ```sh
  POST /website/blog/1/_update
  {
     "doc" : {
        "tags" : [ "testing" ],
        "views": 0
     }
  }
  ```

###### 使用脚本部分更新文档

- 通过使用脚本可以在 `update` API中用来改变 `_source` 的字段内容；

  ```sh
  POST /website/blog/1/_update
  {
     "script" : "ctx._source.views+=1"
  }
  ```

- 通过使用脚本给 `tags` 数组添加一个新的标签；

  ```sh
  POST /website/blog/1/_update
  {
     "script" : "ctx._source.tags+=new_tag",
     "params" : {
        "new_tag" : "search"
     }
  }
  ```

- ###### 更新和冲突

- `update` API 在 *检索* 步骤时**检索得到文档当前的 `_version` 号**，并传递版本号到 *重建索引* 步骤的 `index` 请求。 如果另一个进程修改了处于检索和重新索引步骤之间的文档，**那么 `_version`号将不匹配，更新请求将会失败。**

- 对于部分更新的很多使用场景，文档已经被改变也没有关系，可以**通过设置参数 `retry_on_conflict` 来自动完成**， 这个参数规定了失败之前 `update` 应该重试的次数，它的默认值为 `0` 。

- ```sh
  POST /website/pageviews/1/_update?retry_on_conflict=5
  {
     "script" : "ctx._source.views+=1",
     "upsert": {
         "views": 0
     }
  }
  ```

##### 07：创建新的文档 [_create]

- `_index`、`_type`、`_id` 三者唯一确定一个文档；

- 使用 **POST 方法让 ES 自动生成唯一ID**;

  ```sql
  POST /website/blog/
  { ... }
  ```

- 如果**自己设置 `_id`** ，那么我们必须告诉 Elasticsearch ，只有在相同的 `_index` 、 `_type` 和 `_id` 不存在时才接受我们的索引请求。

  - 第一种方法**使用 `op_type`操作类型** ：
  
    ```js
    PUT /website/blog/123?op_type=create
    { ... }
    ```
  
  - 第二种方法是**在 URL 末端使用 `/_create`** :
  
    ```js
    PUT /website/blog/123/_create
    { ... }
    ```
  
  - 若是创建新文档，则返回201，反之返回 409 Conflict;

##### 08：删除文档 

- 查看返回结果，版本号已经增加
- 如果没有找到文档将返回`404 Not Found` 的响应码；
- 即使文档不存在not_found， **`_version` 值仍然会增加**。这是 Elasticsearch 内部记录本的一部分，用来**确保这些改变在跨多节点时以正确的顺序执行**。
- 类似更新整个文档中提到的，删除文档不会立即将文档从磁盘中删除，只是**将文档标记为已删除状态**。随着你不断的索引更多的数据，Elasticsearch **将会在后台清理标记为已删除的文档。**

```sql
DELETE /website/blog/123
```

##### 09：处理冲突

- ES并发写
  - 当我们使用 `index` API 更新文档 ，可以一次性读取原始文档，做我们的修改，然后**重新索引整个文档。 最近的索引请求将获胜**：无论最后哪一个文档被索引，都将被唯一存储在 Elasticsearch 中。如果其他人同时更改这个文档，他们的更改将丢失。
- 在数据库中，有两种通用的方法**确保在并发更新时修改不丢失**
  - 悲观锁
  - 乐观锁
- Elasticsearch 是**分布式的**，当文档创建、更新或删除时， 新版本的文档必须复制到集群中的其他节点。Elasticsearch 也是**异步和并发的**，这意味着**这些复制请求被并行发送，并且到达目的地时可能顺序是乱的。** 
- 每个文档都有一个 `_version` （版本）号，当文档被修改时版本号递增。 **ES 通过这个 `_version` 号来确保变更以正确顺序得到执行**。如果旧版本的文档在新版本之后到达，它可以被简单的忽略。

###### 通过外部系统使用版本控制

- 常见的设置是**使用其它数据库作为主要的数据存储，使用 Elasticsearch 做数据检索**， 这意味着主数据库的所有更改发生时都需要被复制到 Elasticsearch ，如果多个进程负责这一数据同步，可能存在问题；

- 如果你的主数据库已经有了**版本号 或一个能作为版本号的字段值**比如 `timestamp` — 那么你就可以在 ES 中通过增加 `version_type=external` 查询字符串的方式重用这些相同的版本号， 版本号必须是大于零的整数， 且小于 `9.2E+18` 一个 Java 中 `long` 类型的正值。

- 外部版本号的处理方式是 **Elasticsearch 检查当前 `_version` 是否 *小于* 指定的版本号，小于则请求成功。 若请求成功，外部的版本号作为文档的新 `_version` 进行存储**。

- 例如，要创建一个新的具有外部版本号 `5` 的博客文章，我们可以按以下方法进行：

- 存在一个问题：旧文档先于新文档到达，并且版本号都是大于当前版本；

  ```sh
  # PUT /website/blog/2?version=5&version_type=external
  {
    "title": "My first external blog entry",
    "text":  "Starting to get the hang of this..."
  }
  ```

##### 10：取回多个文档 [_mget]

- 使用 *multi-get* 或者 `mget` API 来将这些检索请求放在一个请求中，将比逐个文档请求更快地检索到全部文档;

- `mget` API **要求有一个 `docs` 数组作为参数**，每个元素包含需要检索文档的元数据， 包括 `_index` 、 `_type` 和 `_id` 。如果你**想检索一个或者多个特定的字段，那么你可以通过 `_source` 参数来指定这些字段的名字**

- ```sh
  # GET /_mget
  {
     "docs" : [
        {
           "_index" : "website",
           "_type" :  "blog",
           "_id" :    2
        },
        {
           "_index" : "website",
           "_type" :  "pageviews",
           "_id" :    1,
           "_source": "views"
        }
     ]
  }
  ```

- 如果想**检索的数据都在相同的 `_index` 中（甚至相同的 `_type` 中），则可以在 URL 中指定默认的 `/_index` 或者默认的 `/_index/_type` 。**

- ```sh
  # GET /website/blog/_mget
  {
     "docs" : [
        { "_id" : 2 },
        { "_type" : "pageviews", "_id" :   1 }
     ]
  }
  ```

- 事实上第二个文档未能找到并不妨碍第一个文档被检索到。每个文档都是单独检索和报告的;

##### 11：代价较小的批量操作

- **bulk API 允许在单个步骤中进行多次 `create` 、 `index` 、 `update` 或 `delete` 请求；**

- 这种格式类似一个有效的单行 JSON 文档流，每行一定要以换行符(`\n`)结尾， *包括最后一行* 。这些换行符被用作一个标记，可以有效分隔行；

- ```sh
  { action: { metadata }}\n
  { request body        }\n
  { action: { metadata }}\n
  { request body        }\n
  ...
  ```

- `metadata` 应该 指定被索引、创建、更新或者删除的文档的 `_index` 、 `_type` 和 `_id` ；

- ```
  { "delete": { "_index": "website", "_type": "blog", "_id": "123" }}
  ```

###### 完整的 bulk 请求；

- 请注意 `delete` 动作不能有请求体,它后面跟着的是另外一个操作；
- 每个**子请求都是独立执行**，因此某个子请求的失败不会对其他子请求的成功与否造成影响。 **如果其中任何子请求失败，最顶层的 `error` 标志被设置为 `true` ，并且在相应的请求报告出错误明细**；

```sh
POST /_bulk
{ "delete": { "_index": "website", "_type": "blog", "_id": "123" }}    
{ "create": { "_index": "website", "_type": "blog", "_id": "123" }}
{ "title":    "My first blog post" }
{ "index":  { "_index": "website", "_type": "blog" }}
{ "title":    "My second blog post" }
{ "update": { "_index": "website", "_type": "blog", "_id": "123", "_retry_on_conflict" : 3} }
{ "doc" : {"title" : "My updated blog post"} }
```

- 为什么 bulk API需要带换行符的奇怪格式，而不是像 mget API一样使用JSON数组？
  - 批量中每个引用的文档属于不同的主分片，每个分片可能被分布于集群中的某个节点上。这意味着批量中的每个操作(action)需要被转发到对应的分片和节点上，如果是Json数组，需要在**内存中执行一些convert操作；**
  - 它使用**换行符识别和解析action/metadata行**，以决定哪些分片来处理这个请求。

###### 最佳节点【sweetspot】

- 整个批量请求都需要**由接收到请求的节点加载到内存中**，因此该请求越大，其他请求所能获得的内存就越少。 批量请求的大小有**一个最佳值**，大于这个值，性能将不再提升，甚至会下降。 但是最佳值不是一个固定的值。它完全取决于硬件、文档的大小和复杂度、索引和搜索的负载的整体情况。

##### 12：Term 向量

- 





